{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e13db714",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to /home/kelvin/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-trained word vectors...\n",
      "\n",
      "Loading GPT-2 model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”¹ Original Prompt: Who is king.\n",
      "Replacing 'king' â†’ 'prince'\n",
      "\n",
      "ðŸ”¹ Enriched Prompt: Who is prince .\n"
     ]
    }
   ],
   "source": [
    "# Install required libraries \n",
    "# Install gensim for downloading pre-trained models \n",
    "# !pip install gensim \n",
    "# Install Hugging Face Transformers for NLP pipelines \n",
    "# !pip install transformers \n",
    "# Install NLTK for text preprocessing and tokenization \n",
    "# !pip install nltk \n",
    " \n",
    "# Import libraries \n",
    "import gensim.downloader as api \n",
    "from transformers import pipeline \n",
    "import nltk \n",
    "import string \n",
    "from nltk.tokenize import word_tokenize \n",
    "# Download the 'punkt_tab' resource from NLTK \n",
    "nltk.download('punkt_tab') \n",
    " \n",
    "# Load pre-trained word vectors \n",
    "print(\"Loading pre-trained word vectors...\") \n",
    "word_vectors = api.load(\"glove-wiki-gigaword-100\")  # Load Word2Vec model \n",
    " \n",
    "# Function to replace words in the prompt with their most similar words \n",
    "def replace_keyword_in_prompt(prompt, keyword, word_vectors, topn=1): \n",
    "    \"\"\" \n",
    "                                                          \n",
    "    Replace only the specified keyword in the prompt with its most similar word. \n",
    " \n",
    "    Args: \n",
    "        prompt (str): The original input prompt. \n",
    "        keyword (str): The word to be replaced with a similar word. \n",
    "        word_vectors (gensim.models.KeyedVectors): Pre-trained word embeddings. \n",
    "        topn (int): Number of top similar words to consider (default: 1). \n",
    " \n",
    "    Returns: \n",
    "        str: The enriched prompt with the keyword replaced. \n",
    "    \"\"\" \n",
    "    words = word_tokenize(prompt)  # Tokenize the prompt into words \n",
    "    enriched_words = [] \n",
    " \n",
    "    for word in words: \n",
    "        cleaned_word = word.lower().strip(string.punctuation)  # Normalize word \n",
    "         \n",
    "        if cleaned_word == keyword.lower():  # Replace only if it matches the keyword \n",
    "            try: \n",
    "                # Retrieve similar word \n",
    "                similar_words = word_vectors.most_similar(cleaned_word, topn=topn) \n",
    "                if similar_words: \n",
    "                    replacement_word = similar_words[0][0]  # Choose the most similar word \n",
    "                    print(f\"Replacing '{word}' â†’ '{replacement_word}'\") \n",
    "                    enriched_words.append(replacement_word) \n",
    "                    continue  # Skip appending the original word \n",
    "            except KeyError: \n",
    "                print(f\"'{keyword}' not found in the vocabulary. Using original word.\") \n",
    " \n",
    "                                                          \n",
    " \n",
    "        enriched_words.append(word)  # Keep original if no replacement was made \n",
    " \n",
    "    enriched_prompt = \" \".join(enriched_words) \n",
    "    print(f\"\\nðŸ”¹ Enriched Prompt: {enriched_prompt}\") \n",
    "    return enriched_prompt \n",
    " \n",
    "# Load an open-source Generative AI model (GPT-2) \n",
    "print(\"\\nLoading GPT-2 model...\") \n",
    "generator = pipeline(\"text-generation\", model=\"gpt2\") \n",
    " \n",
    "# Function to generate responses using the Generative AI model \n",
    "def generate_response(prompt, max_length=100): \n",
    "    try: \n",
    "        response = generator(prompt, max_length=max_length, num_return_sequences=1) \n",
    "        return response[0]['generated_text'] \n",
    "    except Exception as e: \n",
    "        print(f\"Error generating response: {e}\") \n",
    "        return None \n",
    " \n",
    "# Example original prompt \n",
    "original_prompt = \"Who is king.\" \n",
    "print(f\"\\nðŸ”¹ Original Prompt: {original_prompt}\") \n",
    " \n",
    "# Retrieve similar words for key terms in the prompt \n",
    "key_term = \"king\" \n",
    " \n",
    "# Enrich the original prompt \n",
    "enriched_prompt = replace_keyword_in_prompt(original_prompt,key_term, \n",
    "word_vectors) \n",
    "                                                          \n",
    " \n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3524d26a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating response for the original prompt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original Prompt Response:\n",
      "Who is king. The king who is not king. Let the good things have no authority.\"\n",
      "\n",
      "As we read below, the same was the case with the words \"the people will not have authority but are rulers of nations,\" which has been attributed to God \"in a second coming.\" In that context, \"may God give authority not just to the people but to every nation\" (Gal 3:19).\n",
      "\n",
      "It is hard to agree on what is at all true. It's\n",
      "\n",
      "Generating response for the enriched prompt...\n",
      "\n",
      "Enriched Prompt Response:\n",
      "Who is prince ...\" \"My lord,\" \"My lord,\" \"My lord,\" \"My lord,\" \"My lord,\" \"My lord,\" \"My lord,\" \"My lord,\" \"My lord,\" \"My lord,\" \"My lord,\" \"My lord,\" \"My lord,\" \"My lord,\" \"My lord,\" \"My lord,\" \"My lord,\" \"My lord,\" \"My lord,\" \"My lord\" \"My lord\" \"My lord\" \"My lord\" \"My\n"
     ]
    }
   ],
   "source": [
    "# Generate responses for the original and enriched prompts \n",
    "print(\"\\nGenerating response for the original prompt...\") \n",
    "original_response = generate_response(original_prompt) \n",
    "print(\"\\nOriginal Prompt Response:\") \n",
    "print(original_response) \n",
    " \n",
    "print(\"\\nGenerating response for the enriched prompt...\") \n",
    "enriched_response = generate_response(enriched_prompt) \n",
    "print(\"\\nEnriched Prompt Response:\") \n",
    "print(enriched_response) \n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f7195ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Comparison of Responses:\n",
      "\n",
      "Original Prompt Response Length: 400\n",
      "Enriched Prompt Response Length: 271\n",
      "\n",
      "Original Prompt Response Detail: 6\n",
      "Enriched Prompt Response Detail: 3\n"
     ]
    }
   ],
   "source": [
    "# Compare the outputs \n",
    "print(\"\\nComparison of Responses:\") \n",
    "print(\"\\nOriginal Prompt Response Length:\", len(original_response)) \n",
    "print(\"Enriched Prompt Response Length:\", len(enriched_response)) \n",
    "print(\"\\nOriginal Prompt Response Detail:\", original_response.count(\".\")) \n",
    "print(\"Enriched Prompt Response Detail:\", enriched_response.count(\".\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a960d666",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "linux-deep-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
