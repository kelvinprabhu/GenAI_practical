{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13db714",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-25 15:06:08.890771: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1745593569.104444   18446 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1745593569.156196   18446 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1745593569.574201   18446 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1745593569.574321   18446 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1745593569.574325   18446 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1745593569.574328   18446 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-04-25 15:06:09.615390: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "[nltk_data] Downloading package punkt_tab to /home/kelvin/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-trained word vectors...\n",
      "[==================================================] 100.0% 128.1/128.1MB downloaded\n",
      "\n",
      "Loading GPT-2 model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df319464f3f846f29536596790b52a66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37e3e16c5c1a43f38a0386098e479ce3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05288c1f42c342479094e63b3e399e5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "052ab2b6432f4e16af29fadaa41f3e8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f799d0b0d244c3b90f3a1d18e16c6c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa0453f7c4764f89bd7be31a0a2dfc56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3db7bad8aec24ea7b18323fa88160392",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”¹ Original Prompt: Who is king.\n",
      "Replacing 'king' â†’ 'prince'\n",
      "\n",
      "ðŸ”¹ Enriched Prompt: Who is prince .\n"
     ]
    }
   ],
   "source": [
    "# Install required libraries \n",
    "# Install gensim for downloading pre-trained models \n",
    "# !pip install gensim \n",
    "# Install Hugging Face Transformers for NLP pipelines \n",
    "# !pip install transformers \n",
    "# Install NLTK for text preprocessing and tokenization \n",
    "# !pip install nltk \n",
    " \n",
    "# Import libraries \n",
    "import gensim.downloader as api \n",
    "from transformers import pipeline \n",
    "import nltk \n",
    "import string \n",
    "from nltk.tokenize import word_tokenize \n",
    "# Download the 'punkt_tab' resource from NLTK \n",
    "nltk.download('punkt_tab') \n",
    " \n",
    "# Load pre-trained word vectors \n",
    "print(\"Loading pre-trained word vectors...\") \n",
    "word_vectors = api.load(\"glove-wiki-gigaword-100\")  # Load Word2Vec model \n",
    " \n",
    "# Function to replace words in the prompt with their most similar words \n",
    "def replace_keyword_in_prompt(prompt, keyword, word_vectors, topn=1): \n",
    "    \"\"\" \n",
    "                                                          \n",
    "    Replace only the specified keyword in the prompt with its most similar word. \n",
    " \n",
    "    Args: \n",
    "        prompt (str): The original input prompt. \n",
    "        keyword (str): The word to be replaced with a similar word. \n",
    "        word_vectors (gensim.models.KeyedVectors): Pre-trained word embeddings. \n",
    "        topn (int): Number of top similar words to consider (default: 1). \n",
    " \n",
    "    Returns: \n",
    "        str: The enriched prompt with the keyword replaced. \n",
    "    \"\"\" \n",
    "    words = word_tokenize(prompt)  # Tokenize the prompt into words \n",
    "    enriched_words = [] \n",
    " \n",
    "    for word in words: \n",
    "        cleaned_word = word.lower().strip(string.punctuation)  # Normalize word \n",
    "         \n",
    "        if cleaned_word == keyword.lower():  # Replace only if it matches the keyword \n",
    "            try: \n",
    "                # Retrieve similar word \n",
    "                similar_words = word_vectors.most_similar(cleaned_word, topn=topn) \n",
    "                if similar_words: \n",
    "                    replacement_word = similar_words[0][0]  # Choose the most similar word \n",
    "                    print(f\"Replacing '{word}' â†’ '{replacement_word}'\") \n",
    "                    enriched_words.append(replacement_word) \n",
    "                    continue  # Skip appending the original word \n",
    "            except KeyError: \n",
    "                print(f\"'{keyword}' not found in the vocabulary. Using original word.\") \n",
    " \n",
    "                                                          \n",
    " \n",
    "        enriched_words.append(word)  # Keep original if no replacement was made \n",
    " \n",
    "    enriched_prompt = \" \".join(enriched_words) \n",
    "    print(f\"\\nðŸ”¹ Enriched Prompt: {enriched_prompt}\") \n",
    "    return enriched_prompt \n",
    " \n",
    "# Load an open-source Generative AI model (GPT-2) \n",
    "print(\"\\nLoading GPT-2 model...\") \n",
    "generator = pipeline(\"text-generation\", model=\"gpt2\") \n",
    " \n",
    "# Function to generate responses using the Generative AI model \n",
    "def generate_response(prompt, max_length=100): \n",
    "    try: \n",
    "        response = generator(prompt, max_length=max_length, num_return_sequences=1) \n",
    "        return response[0]['generated_text'] \n",
    "    except Exception as e: \n",
    "        print(f\"Error generating response: {e}\") \n",
    "        return None \n",
    " \n",
    "# Example original prompt \n",
    "original_prompt = \"Who is king.\" \n",
    "print(f\"\\nðŸ”¹ Original Prompt: {original_prompt}\") \n",
    " \n",
    "# Retrieve similar words for key terms in the prompt \n",
    "key_term = \"king\" \n",
    " \n",
    "# Enrich the original prompt \n",
    "enriched_prompt = replace_keyword_in_prompt(original_prompt,key_term, \n",
    "word_vectors) \n",
    "                                                          \n",
    " \n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3524d26a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating response for the original prompt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original Prompt Response:\n",
      "Who is king. And, who is king's father.\"\n",
      "\n",
      "2. And it came to pass in their case, when they were on the morrow, the sons of King David were taken up to him and he healed them; and he bore them many children. And the Lord said unto them, Behold, my servant Daniel, my son, and my servant Solomon the son of David, this day I will give you that shall be your king: and he said unto them, Come\n",
      "\n",
      "Generating response for the enriched prompt...\n",
      "\n",
      "Enriched Prompt Response:\n",
      "Who is prince ....\"... \"He who can bear this name will live forever.... \"\n",
      "\n",
      "[The following line is taken aloud from the Gospel, \"For the Lord knows thy Son from the beginning, that is, until this day,\" and the words in \"For.\"]\n",
      "\n",
      "This, then, is the word of the Lord, not of Adam, or of the Son, but of Christ, the Eternal, the Most Heaven, and the whole\n"
     ]
    }
   ],
   "source": [
    "# Generate responses for the original and enriched prompts \n",
    "print(\"\\nGenerating response for the original prompt...\") \n",
    "original_response = generate_response(original_prompt) \n",
    "print(\"\\nOriginal Prompt Response:\") \n",
    "print(original_response) \n",
    " \n",
    "print(\"\\nGenerating response for the enriched prompt...\") \n",
    "enriched_response = generate_response(enriched_prompt) \n",
    "print(\"\\nEnriched Prompt Response:\") \n",
    "print(enriched_response) \n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f7195ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Comparison of Responses:\n",
      "\n",
      "Original Prompt Response Length: 408\n",
      "Enriched Prompt Response Length: 334\n",
      "\n",
      "Original Prompt Response Detail: 2\n",
      "Enriched Prompt Response Detail: 11\n"
     ]
    }
   ],
   "source": [
    "# Compare the outputs \n",
    "print(\"\\nComparison of Responses:\") \n",
    "print(\"\\nOriginal Prompt Response Length:\", len(original_response)) \n",
    "print(\"Enriched Prompt Response Length:\", len(enriched_response)) \n",
    "print(\"\\nOriginal Prompt Response Detail:\", original_response.count(\".\")) \n",
    "print(\"Enriched Prompt Response Detail:\", enriched_response.count(\".\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a960d666",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "linux-deep-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
